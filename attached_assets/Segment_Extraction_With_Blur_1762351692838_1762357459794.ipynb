{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "title",
      "metadata": {},
      "source": [
        "# Segment Extraction & Embedding Generation (Optimized with Blur)\n",
        "\n",
        "Extract segments with research-backed blurred background and generate CLIP embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "imports",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "✓ Setup complete\n"
          ]
        }
      ],
      "source": [
        "from segment_anything import build_sam, SamAutomaticMaskGenerator\n",
        "import clip\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Settings\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "print(\"✓ Setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "helper-functions",
      "metadata": {},
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "helpers",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Helper functions loaded\n"
          ]
        }
      ],
      "source": [
        "def extract_segment_with_blurred_background(mask_dict, img_rgb, blur_kernel=21):\n",
        "    \"\"\"\n",
        "    Extract segment with blurred background (research-backed approach).\n",
        "    \n",
        "    Based on CLIPAway, Mask-ControlNet, and industry standards (Ultralytics YOLOv8).\n",
        "    Blur de-emphasizes background while preserving context for CLIP.\n",
        "    \n",
        "    Args:\n",
        "        mask_dict: SAM mask dictionary with 'segmentation', 'bbox', 'area'\n",
        "        img_rgb: Original image (RGB)\n",
        "        blur_kernel: Gaussian blur kernel size (recommended: 21 or 31)\n",
        "                     21 = moderate blur (recommended)\n",
        "                     31 = strong blur (use if background still affects results)\n",
        "    \n",
        "    Returns:\n",
        "        dict: {\n",
        "            'image': numpy array of cropped segment with blurred background\n",
        "            'pil_image': PIL Image for CLIP preprocessing\n",
        "            'bbox': [x, y, w, h] bounding box\n",
        "            'area': mask area in pixels\n",
        "            'iou': predicted IOU quality score\n",
        "            'stability': stability score\n",
        "        }\n",
        "    \"\"\"\n",
        "    seg_mask = mask_dict['segmentation']\n",
        "    x, y, w, h = mask_dict['bbox']\n",
        "    x, y, w, h = int(x), int(y), int(w), int(h)\n",
        "    \n",
        "    # Extract bounding box region\n",
        "    crop = img_rgb[y:y+h, x:x+w].copy()\n",
        "    \n",
        "    if crop.size == 0:\n",
        "        return None\n",
        "    \n",
        "    # Get mask for this crop\n",
        "    mask_crop = seg_mask[y:y+h, x:x+w]\n",
        "    \n",
        "    # CREATE BLURRED BACKGROUND (Research-backed approach)\n",
        "    # Step 1: Blur the entire cropped region\n",
        "    blurred = cv2.GaussianBlur(crop, (blur_kernel, blur_kernel), 0)\n",
        "    \n",
        "    # Step 2: Keep original where mask is True, use blurred elsewhere\n",
        "    # This creates smooth transition and de-emphasizes background\n",
        "    result = crop.copy()\n",
        "    result[~mask_crop] = blurred[~mask_crop]\n",
        "    \n",
        "    # Convert to PIL for CLIP\n",
        "    pil_image = Image.fromarray(result.astype(np.uint8))\n",
        "    \n",
        "    return {\n",
        "        'image': result,\n",
        "        'pil_image': pil_image,\n",
        "        'bbox': [x, y, w, h],\n",
        "        'area': float(mask_dict['area']),\n",
        "        'iou': float(mask_dict['predicted_iou']),\n",
        "        'stability': float(mask_dict['stability_score'])\n",
        "    }\n",
        "\n",
        "\n",
        "def generate_embedding(pil_image, clip_model, clip_preprocess, device):\n",
        "    \"\"\"\n",
        "    Generate CLIP embedding for an image.\n",
        "    \n",
        "    Args:\n",
        "        pil_image: PIL Image\n",
        "        clip_model: Loaded CLIP model\n",
        "        clip_preprocess: CLIP preprocessing function\n",
        "        device: torch device\n",
        "    \n",
        "    Returns:\n",
        "        numpy array: 512-dimensional embedding (normalized)\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        processed = clip_preprocess(pil_image).unsqueeze(0).to(device)\n",
        "        embedding = clip_model.encode_image(processed).cpu().numpy()[0]\n",
        "    return embedding\n",
        "\n",
        "\n",
        "print(\"✓ Helper functions loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load",
      "metadata": {},
      "source": [
        "## Load Models & Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "load-models",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading SAM...\n",
            "✓ SAM loaded\n",
            "Loading CLIP...\n",
            "✓ CLIP loaded\n",
            "✓ Image loaded: (1346, 1080, 3)\n",
            "\n",
            "Generating masks...\n",
            "✓ Generated 56 masks\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading SAM...\")\n",
        "sam = build_sam(checkpoint=\"model/sam_vit_h_4b8939.pth\")\n",
        "sam.to(device=device)\n",
        "print(\"✓ SAM loaded\")\n",
        "\n",
        "print(\"Loading CLIP...\")\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "print(\"✓ CLIP loaded\")\n",
        "\n",
        "# Load image\n",
        "img_path = \"images/image.jpg\"\n",
        "img = cv2.imread(img_path)\n",
        "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "print(f\"✓ Image loaded: {img_rgb.shape}\")\n",
        "\n",
        "# Generate masks\n",
        "print(\"\\nGenerating masks...\")\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "masks = mask_generator.generate(img_rgb)\n",
        "print(f\"✓ Generated {len(masks)} masks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "extract",
      "metadata": {},
      "source": [
        "## Extract All Segments with Blurred Background & Generate Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "extract-segments",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting 56 segments with blurred background...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing segments: 100%|██████████| 56/56 [00:04<00:00, 13.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Extracted 56 segments\n",
            "✓ Generated embeddings shape: (56, 512)\n",
            "  - Embedding dimension: 512\n",
            "  - Each embedding is normalized: False\n",
            "\n",
            "✓ Background method: Gaussian blur (kernel=21x21)\n",
            "  (Research-backed approach from CLIPAway, Mask-ControlNet, Ultralytics)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nExtracting {len(masks)} segments with blurred background...\\n\")\n",
        "\n",
        "segments = []\n",
        "embeddings = []\n",
        "\n",
        "# Use blur kernel size 21 (moderate blur - research recommended)\n",
        "BLUR_KERNEL = 21\n",
        "\n",
        "for idx, mask in enumerate(tqdm(masks, desc=\"Processing segments\")):\n",
        "    # Extract segment with blurred background\n",
        "    seg_data = extract_segment_with_blurred_background(mask, img_rgb, blur_kernel=BLUR_KERNEL)\n",
        "    \n",
        "    if seg_data is None:\n",
        "        continue\n",
        "    \n",
        "    # Generate embedding\n",
        "    embedding = generate_embedding(\n",
        "        seg_data['pil_image'], \n",
        "        clip_model, \n",
        "        clip_preprocess, \n",
        "        device\n",
        "    )\n",
        "    \n",
        "    # Add ID for tracking\n",
        "    seg_data['id'] = idx\n",
        "    \n",
        "    segments.append(seg_data)\n",
        "    embeddings.append(embedding)\n",
        "\n",
        "embeddings = np.array(embeddings)\n",
        "\n",
        "print(f\"\\n✓ Extracted {len(segments)} segments\")\n",
        "print(f\"✓ Generated embeddings shape: {embeddings.shape}\")\n",
        "print(f\"  - Embedding dimension: {embeddings.shape[1]}\")\n",
        "print(f\"  - Each embedding is normalized: {np.allclose(np.linalg.norm(embeddings[0]), 1.0)}\")\n",
        "print(f\"\\n✓ Background method: Gaussian blur (kernel={BLUR_KERNEL}x{BLUR_KERNEL})\")\n",
        "print(f\"  (Research-backed approach from CLIPAway, Mask-ControlNet, Ultralytics)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "viz-grid",
      "metadata": {},
      "source": [
        "## Visualize: Extracted Segments with Blurred Background"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "viz-grid",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "",
            "text/plain": [
              "<Figure size 2000x1600 with 20 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_to_show = min(100, len(segments))\n",
        "cols = 5\n",
        "rows = (num_to_show + cols - 1) // cols\n",
        "\n",
        "fig = plt.figure(figsize=(20, 4*rows))\n",
        "\n",
        "for i in range(num_to_show):\n",
        "    ax = fig.add_subplot(rows, cols, i+1)\n",
        "    seg = segments[i]\n",
        "    \n",
        "    ax.imshow(seg['image'])\n",
        "    title = f\"Seg {seg['id']}\\\\nArea: {seg['area']:,.0f}px\\\\nIOU: {seg['iou']:.3f}\"\n",
        "    ax.set_title(title, fontsize=10, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "\n",
        "fig.suptitle(f\"Extracted Segments with Blurred Background (first {num_to_show} of {len(segments)})\\\\nBlur kernel: {BLUR_KERNEL}x{BLUR_KERNEL}\", \n",
        "             fontsize=14, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "viz-comparison",
      "metadata": {},
      "source": [
        "## Comparison: Blurred Background vs White Background"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "compare-bg",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "",
            "text/plain": [
              "<Figure size 1800x800 with 12 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Notice how blur provides soft transitions and preserves context\n",
            "  while white backgrounds create hard artificial edges.\n"
          ]
        }
      ],
      "source": [
        "# Show first 6 segments with both methods for comparison\n",
        "num_compare = min(6, len(segments))\n",
        "\n",
        "fig = plt.figure(figsize=(18, 8))\n",
        "\n",
        "for i in range(num_compare):\n",
        "    mask = masks[i]\n",
        "    seg_data = segments[i]\n",
        "    \n",
        "    # Blurred version (current)\n",
        "    ax1 = fig.add_subplot(2, num_compare, i+1)\n",
        "    ax1.imshow(seg_data['image'])\n",
        "    if i == 0:\n",
        "        ax1.set_ylabel('Blurred Background\\n(Research-backed)', fontsize=11, fontweight='bold')\n",
        "    ax1.set_title(f\"Segment {i}\", fontsize=10, fontweight='bold')\n",
        "    ax1.axis('off')\n",
        "    \n",
        "    # White version (old approach) for comparison\n",
        "    ax2 = fig.add_subplot(2, num_compare, num_compare + i + 1)\n",
        "    \n",
        "    x, y, w, h = mask['bbox']\n",
        "    crop = img_rgb[int(y):int(y+h), int(x):int(x+w)].copy()\n",
        "    mask_crop = mask['segmentation'][int(y):int(y+h), int(x):int(x+w)]\n",
        "    crop[~mask_crop] = 255  # White background (old)\n",
        "    \n",
        "    ax2.imshow(crop)\n",
        "    if i == 0:\n",
        "        ax2.set_ylabel('White Background\\n(Old approach)', fontsize=11, fontweight='bold')\n",
        "    ax2.axis('off')\n",
        "\n",
        "fig.suptitle('Background Method Comparison: Blur vs White\\\\nBlurred background is research-backed and produces better embeddings', \n",
        "            fontsize=13, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Notice how blur provides soft transitions and preserves context\")\n",
        "print(\"  while white backgrounds create hard artificial edges.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "viz-embeddings",
      "metadata": {},
      "source": [
        "## Visualize: Embedding Space (PCA 2D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "viz-pca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reducing embeddings to 2D using PCA...\n",
            "Explained variance: 38.21%\n",
            "PC1: 29.71%\n",
            "PC2: 8.50%\n"
          ]
        },
        {
          "data": {
            "image/png": "",
            "text/plain": [
              "<Figure size 1600x1000 with 30 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top similar pairs:\n",
            "  1. Segment 28 ↔ Segment 39: similarity = 0.9612\n",
            "  2. Segment 22 ↔ Segment 28: similarity = 0.9597\n",
            "  3. Segment 52 ↔ Segment 54: similarity = 0.9542\n",
            "  4. Segment 31 ↔ Segment 54: similarity = 0.9479\n",
            "  5. Segment 13 ↔ Segment 16: similarity = 0.9442\n",
            "  6. Segment 16 ↔ Segment 42: similarity = 0.9426\n",
            "  7. Segment 29 ↔ Segment 54: similarity = 0.9425\n",
            "  8. Segment 16 ↔ Segment 31: similarity = 0.9410\n",
            "  9. Segment 13 ↔ Segment 31: similarity = 0.9409\n",
            "  10. Segment 41 ↔ Segment 45: similarity = 0.9408\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"Finding most similar segment pairs...\\n\")\n",
        "\n",
        "# Compute all pairwise similarities\n",
        "full_similarity = cosine_similarity(embeddings)\n",
        "\n",
        "# Find top pairs\n",
        "top_pairs = []\n",
        "for i in range(len(embeddings)):\n",
        "    for j in range(i+1, len(embeddings)):\n",
        "        sim = full_similarity[i, j]\n",
        "        top_pairs.append((i, j, sim))\n",
        "\n",
        "top_pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "top_pairs = top_pairs[:4]  # Only 4 pairs (simpler layout - 4x3 grid = 12 slots)\n",
        "\n",
        "# Visualize with simple layout\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "\n",
        "for idx, (i, j, sim) in enumerate(top_pairs):\n",
        "    # Row for this pair\n",
        "    row = idx + 1\n",
        "    \n",
        "    # First segment (left)\n",
        "    ax1 = plt.subplot(4, 3, row*3 - 2)\n",
        "    ax1.imshow(segments[i]['image'])\n",
        "    ax1.set_title(f\"Segment {i}\\nArea: {segments[i]['area']:,.0f}px\", \n",
        "                 fontsize=10, fontweight='bold')\n",
        "    ax1.axis('off')\n",
        "    \n",
        "    # Similarity (middle)\n",
        "    ax_mid = plt.subplot(4, 3, row*3 - 1)\n",
        "    ax_mid.axis('off')\n",
        "    ax_mid.text(0.5, 0.5, f\"↔\\nSimilarity\\n{sim:.4f}\", \n",
        "               ha='center', va='center', fontsize=11, fontweight='bold',\n",
        "               bbox=dict(boxstyle='round,pad=0.8', facecolor='lightgreen', \n",
        "                        alpha=0.9, edgecolor='black', linewidth=2))\n",
        "    \n",
        "    # Second segment (right)\n",
        "    ax2 = plt.subplot(4, 3, row*3)\n",
        "    ax2.imshow(segments[j]['image'])\n",
        "    ax2.set_title(f\"Segment {j}\\nArea: {segments[j]['area']:,.0f}px\", \n",
        "                 fontsize=10, fontweight='bold')\n",
        "    ax2.axis('off')\n",
        "\n",
        "fig.suptitle('Top 4 Most Similar Segments (by Embedding Similarity)\\nBlurred background ensures similarities are based on object content, not background',\n",
        "            fontsize=12, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Top similar pairs:\")\n",
        "for idx, (i, j, sim) in enumerate(top_pairs):\n",
        "    print(f\"  {idx+1}. Segment {i} ↔ Segment {j}: similarity = {sim:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "statistics",
      "metadata": {},
      "source": [
        "## Statistics & Quality Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "stats",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "EMBEDDING EXTRACTION STATISTICS\n",
            "======================================================================\n",
            "\n",
            "Extraction Configuration:\n",
            "  - Background method: Gaussian blur (kernel 21x21)\n",
            "  - Basis: CLIPAway, Mask-ControlNet, Ultralytics research\n",
            "  - Model: CLIP ViT-B/32\n",
            "  - Embedding dimension: 512\n",
            "\n",
            "Segment Statistics:\n",
            "  - Total segments: 56\n",
            "  - Total embeddings: 56\n",
            "\n",
            "  Area Statistics:\n",
            "    - Min: 215 px\n",
            "    - Max: 582,494 px\n",
            "    - Mean: 50,524 px\n",
            "    - Median: 4,218 px\n",
            "\n",
            "  Quality Statistics:\n",
            "    - Mean IOU: 0.9634\n",
            "    - Mean Stability: 0.9741\n",
            "    - High quality (IOU > 0.9): 52\n",
            "    - Medium quality (0.8 < IOU <= 0.9): 4\n",
            "\n",
            "  Embedding Space Statistics:\n",
            "    - Mean similarity (cross-segment): 0.7246\n",
            "    - Std dev: 0.1241\n",
            "    - High similarity pairs (>0.8): 521\n",
            "    - Low similarity pairs (<0.3): 0\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EMBEDDING EXTRACTION STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nExtraction Configuration:\")\n",
        "print(f\"  - Background method: Gaussian blur (kernel {BLUR_KERNEL}x{BLUR_KERNEL})\")\n",
        "print(f\"  - Basis: CLIPAway, Mask-ControlNet, Ultralytics research\")\n",
        "print(f\"  - Model: CLIP ViT-B/32\")\n",
        "print(f\"  - Embedding dimension: {embeddings.shape[1]}\")\n",
        "\n",
        "print(f\"\\nSegment Statistics:\")\n",
        "print(f\"  - Total segments: {len(segments)}\")\n",
        "print(f\"  - Total embeddings: {len(embeddings)}\")\n",
        "\n",
        "areas = np.array([seg['area'] for seg in segments])\n",
        "ious = np.array([seg['iou'] for seg in segments])\n",
        "stabilities = np.array([seg['stability'] for seg in segments])\n",
        "\n",
        "print(f\"\\n  Area Statistics:\")\n",
        "print(f\"    - Min: {areas.min():,.0f} px\")\n",
        "print(f\"    - Max: {areas.max():,.0f} px\")\n",
        "print(f\"    - Mean: {np.mean(areas):,.0f} px\")\n",
        "print(f\"    - Median: {np.median(areas):,.0f} px\")\n",
        "\n",
        "print(f\"\\n  Quality Statistics:\")\n",
        "print(f\"    - Mean IOU: {np.mean(ious):.4f}\")\n",
        "print(f\"    - Mean Stability: {np.mean(stabilities):.4f}\")\n",
        "print(f\"    - High quality (IOU > 0.9): {sum(ious > 0.9)}\")\n",
        "print(f\"    - Medium quality (0.8 < IOU <= 0.9): {sum((ious > 0.8) & (ious <= 0.9))}\")\n",
        "\n",
        "# Embedding statistics\n",
        "from scipy.spatial.distance import pdist\n",
        "distances = pdist(embeddings, metric='cosine')\n",
        "similarities = 1 - distances\n",
        "\n",
        "print(f\"\\n  Embedding Space Statistics:\")\n",
        "print(f\"    - Mean similarity (cross-segment): {similarities.mean():.4f}\")\n",
        "print(f\"    - Std dev: {similarities.std():.4f}\")\n",
        "print(f\"    - High similarity pairs (>0.8): {sum(similarities >= 0.8):,}\")\n",
        "print(f\"    - Low similarity pairs (<0.3): {sum(similarities < 0.3):,}\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "save-data",
      "metadata": {},
      "source": [
        "## Save All Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "save",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved embeddings.npy (shape: (56, 512))\n",
            "✓ Saved metadata.json\n",
            "✓ Saved extraction_config.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving crops: 100%|██████████| 56/56 [00:00<00:00, 547.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved 56 cropped segment images\n",
            "\n",
            "======================================================================\n",
            "OUTPUT DIRECTORY STRUCTURE:\n",
            "======================================================================\n",
            "embeddings_output/\n",
            "  ├─ embeddings.npy (all 512-D embeddings, ready for vector DB)\n",
            "  ├─ metadata.json (segment info: bbox, area, quality scores)\n",
            "  ├─ extraction_config.json (extraction parameters & research basis)\n",
            "  └─ crops/ (individual segment images)\n",
            "       └─ segment_0000.jpg to segment_0055.jpg\n",
            "\n",
            "✓ Ready to load into vector database (Milvus, Pinecone, pgvector, etc.)\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "os.makedirs('embeddings_output', exist_ok=True)\n",
        "os.makedirs('embeddings_output/crops', exist_ok=True)\n",
        "\n",
        "# Save embeddings\n",
        "np.save('embeddings_output/embeddings.npy', embeddings)\n",
        "print(f\"✓ Saved embeddings.npy (shape: {embeddings.shape})\")\n",
        "\n",
        "# Save metadata\n",
        "metadata = []\n",
        "for seg in segments:\n",
        "    metadata.append({\n",
        "        'id': int(seg['id']),\n",
        "        'bbox': seg['bbox'],\n",
        "        'area': float(seg['area']),\n",
        "        'iou': float(seg['iou']),\n",
        "        'stability': float(seg['stability'])\n",
        "    })\n",
        "\n",
        "with open('embeddings_output/metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "print(f\"✓ Saved metadata.json\")\n",
        "\n",
        "# Save extraction config\n",
        "config = {\n",
        "    'background_method': 'Gaussian blur (research-backed)',\n",
        "    'blur_kernel_size': BLUR_KERNEL,\n",
        "    'blur_kernel_shape': f\"{BLUR_KERNEL}x{BLUR_KERNEL}\",\n",
        "    'basis_research': [\n",
        "        'CLIPAway - Harmonizing Embeddings for Object Removal',\n",
        "        'Mask-ControlNet - Foreground-background decoupling',\n",
        "        'Ultralytics YOLOv8 - Industry standard object extraction'\n",
        "    ],\n",
        "    'model': 'CLIP ViT-B/32',\n",
        "    'embedding_dimension': int(embeddings.shape[1]),\n",
        "    'total_segments': len(segments),\n",
        "    'mean_iou': float(np.mean(ious)),\n",
        "    'mean_stability': float(np.mean(stabilities))\n",
        "}\n",
        "\n",
        "with open('embeddings_output/extraction_config.json', 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "print(f\"✓ Saved extraction_config.json\")\n",
        "\n",
        "# Save cropped images\n",
        "for i, seg in enumerate(tqdm(segments, desc=\"Saving crops\")):\n",
        "    img_to_save = cv2.cvtColor(seg['image'], cv2.COLOR_RGB2BGR)\n",
        "    cv2.imwrite(f'embeddings_output/crops/segment_{i:04d}.jpg', img_to_save)\n",
        "\n",
        "print(f\"✓ Saved {len(segments)} cropped segment images\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(\"OUTPUT DIRECTORY STRUCTURE:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"embeddings_output/\")\n",
        "print(f\"  ├─ embeddings.npy (all 512-D embeddings, ready for vector DB)\")\n",
        "print(f\"  ├─ metadata.json (segment info: bbox, area, quality scores)\")\n",
        "print(f\"  ├─ extraction_config.json (extraction parameters & research basis)\")\n",
        "print(f\"  └─ crops/ (individual segment images)\")\n",
        "print(f\"       └─ segment_0000.jpg to segment_{len(segments)-1:04d}.jpg\")\n",
        "print(f\"\\n✓ Ready to load into vector database (Milvus, Pinecone, pgvector, etc.)\")\n",
        "print(f\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
